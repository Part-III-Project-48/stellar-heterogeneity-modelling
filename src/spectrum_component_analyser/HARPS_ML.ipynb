{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3944bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6171ac4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "from astropy.visualization import quantity_support\n",
    "quantity_support() # required for numpy to not get annoyed when doing e.g. np.stack()\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from spectrum_component_analyser.internals.readers import read_JWST_fits_all_spectra\n",
    "from spectrum_component_analyser.internals.spectrum import spectrum\n",
    "from spectrum_component_analyser.internals.spectral_grid  import spectral_grid\n",
    "\n",
    "\"\"\"\n",
    "one difference between this and main.ipynb is that here the spectra are processed by appending rows into the matrix A, whereas in main.ipynb spectra are appended as _columns_ into the matrix A\n",
    "\"\"\"\n",
    "\n",
    "from itertools import product\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from astropy.visualization import quantity_support\n",
    "quantity_support()\n",
    "import os\n",
    "\n",
    "from spectrum_component_analyser.internals.spectrum import spectrum\n",
    "from spectrum_component_analyser.internals.spectral_grid import spectral_grid\n",
    "from spectrum_component_analyser.helper import calc_result, get_optimality, plot_nicely\n",
    "from spectrum_component_analyser.internals.readers import read_HARPS_fits\n",
    "\n",
    "external_spectrum_path = Path(\"../../assets/ADP.2016-02-04T01_02_52.843.fits\")\n",
    "script_dir = os.getcwd()  # usually the folder where notebook is running\")\n",
    "wavelength_grid_absolute_path = (script_dir / external_spectrum_path).resolve()\n",
    "\n",
    "spectrum_to_decompose : spectrum = read_HARPS_fits(wavelength_grid_absolute_path, INTEGRATION_INDEX=0, verbose=False)\n",
    "spectrum_to_decompose.plot()\n",
    "\n",
    "mask = np.isfinite(spectrum_to_decompose.Fluxes)\n",
    "\n",
    "spectrum_to_decompose = spectrum_to_decompose[mask]\n",
    "\n",
    "print(\"reading in hdf5\")\n",
    "spectral_grid_relative_path = Path(\"../../assets/HARPS_convolved_spectral_grid.hdf5\")\n",
    "spectral_grid_absolute_path = (script_dir / spectral_grid_relative_path).resolve()\n",
    "spec_grid : spectral_grid = spectral_grid.from_hdf5(absolute_path=spectral_grid_absolute_path)\n",
    "lookup_table = spec_grid.to_lookup_table()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa344510",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read in spectrum to decompose - use it to determine the following \n",
    "spectrum_num_points = len(spectrum_to_decompose)\n",
    "\n",
    "# read in data cube - use it to determine the following\n",
    "number_phoenix_spectra : int = len(spec_grid.T_effs) * len(spec_grid.FeHs) * len(spec_grid.Log_gs) # number of spectra we'll use for train/val\n",
    "\n",
    "# so each ROW in X,A is 1 graph\n",
    "# and the columns of X are just the graph values (ys)\n",
    "# and the columns of X are the different parameters )\n",
    "\n",
    "# effectively we're doing the same thing as in main.ipynb, but without the horrendous bodge for working out what the weights are\n",
    "# here, we're being systematic about the weights and their meaning by appending them all to a matrix (# of PHOENIX spectra rows x 3 columns) beforehand\n",
    "y_curves = []\n",
    "# params is a list of lists of the form [T_eff, FeH, log_g]\n",
    "params = []\n",
    "\n",
    "import astropy.units as u\n",
    "\n",
    "def get_spectra_and_params(T_eff, FeH, log_g, mask) -> tuple[np.array, np.array]:\n",
    "    fluxes = lookup_table[T_eff, FeH, log_g][mask]\n",
    "    params = [T_eff.value, FeH, log_g]\n",
    "    \n",
    "    return fluxes, params\n",
    "\n",
    "fluxes_and_params = Parallel(n_jobs=-1, prefer=\"threads\")(\n",
    "    delayed(get_spectra_and_params)(T_eff, FeH, log_g, mask=mask) for T_eff, FeH, log_g in tqdm(product(spec_grid.T_effs, spec_grid.FeHs, spec_grid.Log_gs), total=len(spec_grid.T_effs) * len(spec_grid.FeHs) * len(spec_grid.Log_gs), desc=\"appending values from data cube to y_curves, params lists\")\n",
    "    )\n",
    "\n",
    "y_curves, params = zip(*fluxes_and_params)  # each element of results is (y_curve, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281562dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_curves[:100])\n",
    "print(params[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1ead35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this block we combine some y curves of different parameters to mimic spots\n",
    "\n",
    "combination_ys = []\n",
    "combination_parameters = []\n",
    "\n",
    "number_of_combined_curves_to_generate : int = int(.1*len(y_curves))\n",
    "\n",
    "number_of_components_to_use = 1\n",
    "\n",
    "number_parameters : int = number_of_components_to_use * 4 # (weight, T_eff, FeH, log_g) x number_of_components_to_use, weight_a, weight_b\n",
    "\n",
    "import random\n",
    "\n",
    "for i in tqdm(range(number_of_combined_curves_to_generate), total=number_of_combined_curves_to_generate):\n",
    "    # or could use e.g. num = random.uniform(0, 0.3) for a uniform float between 0 and 0.3\n",
    "    weights = np.random.dirichlet(alpha=np.ones(number_of_components_to_use))  # shape (n,)\n",
    "\n",
    "    weights[0] = 0.999\n",
    "    weights /= np.sum(weights)\n",
    "    \n",
    "    y_combination = np.zeros_like(y_curves[0]) # length of the spectrum\n",
    "    parameters = [] # this will end up being number_of_components_to_use lots of (weight, T_eff, FeH, log_g)\n",
    "    \n",
    "    for weight in weights:\n",
    "        spectrum_index = random.randint(0, len(y_curves) - 1)\n",
    "\n",
    "        y_combination += weight * y_curves[spectrum_index]\n",
    "        parameters.extend([weight, *params[spectrum_index]])\n",
    "\n",
    "    # add some noise here too\n",
    "    noise_max_amplitude = max(y_combination.value) / 20\n",
    "    y_combination += np.random.uniform(low=-noise_max_amplitude, high=noise_max_amplitude, size=len(y_combination)) * y_combination[0].unit \n",
    "    \n",
    "    combination_ys.append(y_combination)\n",
    "    combination_parameters.append(parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2483457",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(combination_ys[:2])\n",
    "display(combination_parameters[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c18e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.stack([y_curves for spectrum in data_cube(1...number_phoenix_spectra)])\n",
    "X = np.stack(combination_ys)\n",
    "\n",
    "# A = np.stack([list_of_params_that_describe_curve_1,...,number_parameters])\n",
    "A = np.stack(combination_parameters)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "X_tensor = torch.tensor(X_scaler.fit_transform(X), dtype=torch.float32)\n",
    "\n",
    "A_scaler = StandardScaler()\n",
    "A_tensor = torch.tensor(A_scaler.fit_transform(A), dtype=torch.float32)\n",
    "\n",
    "# X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "# A_tensor = torch.tensor(A, dtype=torch.float32)\n",
    "\n",
    "training_fraction : float = 0.7\n",
    "validation_fraction : float = 1.0 - training_fraction\n",
    "\n",
    "cutoff : int = int(number_of_combined_curves_to_generate * training_fraction)\n",
    "print(number_phoenix_spectra)\n",
    "print(len(X_tensor))\n",
    "X_train, X_val = X_tensor[:cutoff], X_tensor[cutoff:]\n",
    "A_train, A_val = A_tensor[:cutoff], A_tensor[cutoff:]\n",
    "\n",
    "display(X_train)\n",
    "display(A_train)\n",
    "\n",
    "display(X_val)\n",
    "display(A_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9038eb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CurveRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(512, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "model = CurveRegressor(spectrum_num_points, number_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb3f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-4)#, weight_decay=1e-4)\n",
    "\n",
    "max_epochs : int = 1600\n",
    "progress_bar = tqdm(range(max_epochs), desc=f\"Initialising...\")\n",
    "\n",
    "best_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "patience = 10\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    preds = model(X_train)\n",
    "    loss = criterion(preds, A_train)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # validation loss\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        val_loss = criterion(model(X_val), A_val)\n",
    "    \n",
    "    progress_bar.set_description(f\"Epoch {epoch:02d} | train loss: {loss:.4f} | val loss: {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= patience:\n",
    "        print(\"[ML] : early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5003d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from spectrum_component_analyser.helper import TEFF_COLUMN, FEH_COLUMN, LOGG_COLUMN, WAVELENGTH_COLUMN, FLUX_COLUMN\n",
    "\n",
    "INTEGRATION_INDEX_COLUMN : str = \"Integration Index\"\n",
    "\n",
    "WEIGHT_COLUMN : str = \"Weight\"\n",
    "\n",
    "num_samples_mc = 5 # number of Monte Carlo forward passes used for MC-Dropout to estimate uncertainty.\n",
    "\n",
    "model.train()\n",
    "\n",
    "index = 0\n",
    "\n",
    "results = pd.DataFrame()\n",
    "\n",
    "all_observational_spectra = [spectrum_to_decompose]\n",
    "target_name = \"K2-18\"\n",
    "\n",
    "for spectrum_to_decompose in tqdm(all_observational_spectra):\n",
    "    preds = []\n",
    "    mask = np.isfinite(spectrum_to_decompose.Fluxes) # mask that removes np.inf values from spectra - this could be observational spectra dependent; so we have to apply this to the data cube after creating the data cube, not before\n",
    "    spectrum_to_decompose = spectrum_to_decompose[mask]\n",
    "    # scale fluxes by same X_scaler that we used to scale the training data\n",
    "    x_input_scaled = X_scaler.transform(spectrum_to_decompose.Fluxes.reshape(1, -1))\n",
    "    x_tensor = torch.tensor(x_input_scaled, dtype=torch.float32)\n",
    "\n",
    "    # x_tensor = torch.tensor(spectrum_to_decompose.Fluxes, dtype=torch.float32)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_samples_mc):\n",
    "            preds.append(model(x_tensor))\n",
    "\n",
    "    preds = torch.stack(preds) # shape [num_samples_mc, 1, number_parameters]\n",
    "\n",
    "    mean_prediction = preds.mean(dim = 0)\n",
    "    std_prediction = preds.std(dim = 0)\n",
    "\n",
    "    mean_pred_scaled = mean_prediction.detach().numpy()\n",
    "    mean_pred_physical : np.array = A_scaler.inverse_transform(mean_pred_scaled.reshape(1, -1))[0]\n",
    "\n",
    "    std_pred_scaled = std_prediction.detach().numpy()\n",
    "    std_pred_physical = std_pred_scaled * A_scaler.scale_\n",
    "\n",
    "    n_variables = 4\n",
    "\n",
    "    column_names = [WEIGHT_COLUMN, TEFF_COLUMN, FEH_COLUMN, LOGG_COLUMN]\n",
    "\n",
    "    # Convert list to array\n",
    "    arr = np.array(mean_pred_physical)\n",
    "\n",
    "    # Build dictionary dynamically using the string variables\n",
    "    data = {name: arr[i::n_variables] for i, name in enumerate(column_names)}\n",
    "\n",
    "    # Create DataFrame\n",
    "    result = pd.DataFrame(data)\n",
    "    result[INTEGRATION_INDEX_COLUMN] = index\n",
    "    result = result.sort_values(by=column_names[0], ascending=False)\n",
    "\n",
    "    print(result)\n",
    "\n",
    "    plt.scatter(x=result[TEFF_COLUMN][0], y=result[WEIGHT_COLUMN][0], marker=\">\", color=plt.cm.Spectral_r(index/len(all_observational_spectra)), alpha=0.8)\n",
    "    # plt.scatter(x=result[TEFF_COLUMN][1], y=result[WEIGHT_COLUMN][1], marker=\"x\", color=plt.cm.Spectral_r(index/len(all_observational_spectra)), alpha=0.8)\n",
    "    # plt.scatter(x=result[TEFF_COLUMN][2], y=result[WEIGHT_COLUMN][2], marker=\"+\", color=plt.cm.Spectral_r(index/len(all_observational_spectra)), alpha=0.8)\n",
    "    # plt.scatter(x=result[TEFF_COLUMN][3], y=result[WEIGHT_COLUMN][3], marker=\"o\", color=plt.cm.Spectral_r(index/len(all_observational_spectra)), alpha=0.8)\n",
    "    # plt.scatter(x=df[\"T_eff\"][4], y=df[\"Weight\"][4], marker=\"v\", color=plt.cm.Spectral_r(index/len(all_observational_spectra)), alpha=0.8)\n",
    "    index += 1\n",
    "\n",
    "    results = pd.concat([results, result], ignore_index=True)\n",
    "\n",
    "plt.title(f\"JWST observational spectra decomposition for {target_name} for all integration indices\")\n",
    "plt.xlabel(\"$T_\\mathrm{eff}$ / K\")\n",
    "plt.ylabel(\"Weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec82d0d",
   "metadata": {},
   "source": [
    "there are 3 ways to turn our fitted parameters into graphs that we calc a residual from\n",
    "\n",
    "1. interpolate onto PHOENIX grid\n",
    "2. interpolate phoenix graph onto the found parameters\n",
    "3. make the model be a classifier that chooses weights from all the PHOENIX grid options (so make the ML output be discrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d5a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65863e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. interpolate onto PHOENIX grid\n",
    "\n",
    "# lets just do the 1st integration index for now\n",
    "display(results)\n",
    "\n",
    "INTEGRATION_INDEX = 0\n",
    "print(all_observational_spectra[INTEGRATION_INDEX].Fluxes[mask])\n",
    "\n",
    "result_subset = results[results[INTEGRATION_INDEX_COLUMN] == INTEGRATION_INDEX]\n",
    "closest_T_eff = min(spec_grid.T_effs, key=lambda x: abs(x.value - result_subset[TEFF_COLUMN][0]))\n",
    "closest_FeH = min(spec_grid.FeHs, key=lambda x: abs(x - result_subset[FEH_COLUMN][0]))\n",
    "closest_log_g = min(spec_grid.Log_gs, key=lambda x: abs(x - result_subset[LOGG_COLUMN][0]))\n",
    "\n",
    "print(closest_T_eff, closest_FeH, closest_log_g)\n",
    "\n",
    "# get ML prediction from phoenix data\n",
    "normalised_phoenix_spectrum = spec_grid.get_spectrum(closest_T_eff, closest_FeH, closest_log_g)[mask]\n",
    "\n",
    "observational_spectrum = all_observational_spectra[INTEGRATION_INDEX][mask]\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(observational_spectrum.Wavelengths, observational_spectrum.Fluxes, label=f\"observed JWST spectrum for target {target_name}\")\n",
    "plt.plot(normalised_phoenix_spectrum.Wavelengths, normalised_phoenix_spectrum.Fluxes, label=\"found spectrum from ML\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1627dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual = (observational_spectrum.Fluxes - normalised_phoenix_spectrum.Fluxes) / observational_spectrum.Fluxes\n",
    "\n",
    "plt.clf()\n",
    "plt.plot(observational_spectrum.Wavelengths, residual)\n",
    "plt.title(\"(observation - prediction) / observation\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spots-and-faculae-model-YvvEmflj-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
