{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a916007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482b3249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for now, just request a random composite spectrum from facula_and_spot_creator\n",
    "# and try to decompose it - aka can we regenerate the w's\n",
    "\n",
    "# eventually can read in external data or some training data from a large hdf5 file etc\n",
    "\n",
    "from itertools import product\n",
    "from pathlib import Path\n",
    "import astropy\n",
    "from astropy.table import QTable\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from astropy.visualization import quantity_support\n",
    "quantity_support()\n",
    "from tqdm import tqdm\n",
    "import astropy.units as u\n",
    "from scipy.interpolate import interp1d\n",
    "from astropy.units import Quantity\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "\n",
    "from spots_and_faculae_model.spectrum import spectrum\n",
    "from spots_and_faculae_model.readers import read_JWST_fits\n",
    "from spots_and_faculae_model.simpler_spectral_grid import simpler_spectral_grid\n",
    "\n",
    "external_spectrum_path = Path(\"../../assets/MAST_2025-10-26T11_57_04.058Z - LTT-3780/MAST_2025-10-26T11_57_04.058Z/JWST/jw03557004001_04101_00001-seg001_nis_x1dints.fits\")\n",
    "script_dir = os.getcwd()  # usually the folder where notebook is running\")\n",
    "wavelength_grid_absolute_path = (script_dir / external_spectrum_path).resolve()\n",
    "\n",
    "spectrum_to_decompose : spectrum = read_JWST_fits(wavelength_grid_absolute_path, INTEGRATION_INDEX=1)\n",
    "spectrum_to_decompose.plot()\n",
    "print(spectrum_to_decompose)\n",
    "\n",
    "mask = np.isfinite(spectrum_to_decompose.Fluxes)\n",
    "\n",
    "spectrum_to_decompose = spectrum_to_decompose[mask]\n",
    "\n",
    "print(\"reading in hdf5\")\n",
    "spectral_grid_relative_path = Path(\"../../assets/new_spectral_grid.hdf5\")\n",
    "spectral_grid_absolute_path = (script_dir / spectral_grid_relative_path).resolve()\n",
    "spec_grid : simpler_spectral_grid = simpler_spectral_grid.from_hdf5(absolute_path=spectral_grid_absolute_path)\n",
    "lookup_table = spec_grid.to_lookup_table()\n",
    "print(\"finished reading in hdf5\")\n",
    "\n",
    "print(lookup_table[2500 *u.K, 0.0 * u.dimensionless_unscaled, 0.0 * u.dimensionless_unscaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b92b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimality(A, result):\n",
    "    determined_spectrum = spectrum(spectrum_to_decompose.Wavelengths, A @ result.x)\n",
    "    residual = (determined_spectrum.Fluxes - spectrum_to_decompose.Fluxes) / spectrum_to_decompose.Fluxes\n",
    "    rmse = np.sqrt(np.mean(residual**2))\n",
    "    rss  = np.sum(residual**2)\n",
    "\n",
    "    return rmse, rss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a296bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Tuple\n",
    "from scipy.optimize._optimize import OptimizeResult\n",
    "\n",
    "def calc_result(parameter_space, lookup_table, total_number_of_components : int = None, verbose : bool = True) -> Tuple[np.ndarray, OptimizeResult]:\n",
    "    A = np.empty((0, 0))\n",
    "\n",
    "    def force_to_janskys(T_eff : Quantity, FeH : Quantity, log_g : Quantity, wavelengths : Sequence[Quantity], mask):\n",
    "        fluxes = lookup_table[T_eff, FeH, log_g]\n",
    "        return fluxes.to(u.Jy, equivalencies=u.spectral_density(wavelengths))[mask]\n",
    "\n",
    "    normalised_and_converted_spectral_components : list[list[Quantity]] = Parallel(n_jobs=-1, prefer=\"threads\")(\n",
    "        delayed(force_to_janskys)(T_eff, FeH, log_g, spec_grid.Wavelengths, mask) for T_eff, FeH, log_g in tqdm(parameter_space, total=total_number_of_components, desc=\"Appending values to A matrix...\", disable=not verbose)\n",
    "    )\n",
    "\n",
    "    A = np.column_stack(normalised_and_converted_spectral_components)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"minimising\")\n",
    "    \n",
    "    # assume that w \\in [0,1] : but I think this will only be true for real data if normalisation has been done correctly (???)\n",
    "    result : OptimizeResult = sp.optimize.lsq_linear(A, [i.value for i in spectrum_to_decompose.Fluxes], bounds = (0, 1), verbose = 2 if verbose else 0)#, max_iter=2), tol=1e-10, lsmr_tol=1e-5)\n",
    "    \n",
    "    if verbose:\n",
    "        print(result)\n",
    "        print(f\"sum of weights={np.sum(result.x)}\")\n",
    "\n",
    "    return A, result\n",
    "\n",
    "all_parameters = list(product(spec_grid.T_effs, spec_grid.FeHs, spec_grid.Log_gs))\n",
    "\n",
    "total_number_of_components = len(spec_grid.T_effs) * len(spec_grid.FeHs) * len(spec_grid.Log_gs)\n",
    "\n",
    "A, result = calc_result(all_parameters, lookup_table, total_number_of_components)\n",
    "print(f\"residual MSE = {get_optimality(A, result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735ee238",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # # plot some data # # #\n",
    "# dependent on the old spectrum_grid class, but its fine for now (and its just dependent on some arbitrary strings anyway)\n",
    "from spots_and_faculae_model.spectrum_grid import TEFF_COLUMN, FEH_COLUMN, LOGG_COLUMN\n",
    "WEIGHT_COLUMN : str = \"weight\"\n",
    "\n",
    "def plot_nicely(A, result, parameter_space):\n",
    "    result_map = {}\n",
    "    i = 0\n",
    "    for (T_eff, FeH, log_g) in parameter_space:\n",
    "        key = (T_eff, FeH, log_g)\n",
    "        result_map[key] = i\n",
    "        i += 1\n",
    "\n",
    "    hash_map = pd.DataFrame(columns=[TEFF_COLUMN, FEH_COLUMN, LOGG_COLUMN, WEIGHT_COLUMN])\n",
    "    \n",
    "    for (T_eff, FeH, log_g) in parameter_space:\n",
    "        new_row = {TEFF_COLUMN: T_eff, FEH_COLUMN: FeH, LOGG_COLUMN: log_g, WEIGHT_COLUMN: result.x[result_map[(T_eff, FeH, log_g)]]}\n",
    "        hash_map = pd.concat([hash_map, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    print(hash_map.sort_values(WEIGHT_COLUMN, ascending=False).head(10).round(3))\n",
    "\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(15, 15), sharex=True, sharey=True)\n",
    "    axes = axes.ravel()\n",
    "    for i, log_g in enumerate(spec_grid.Log_gs):\n",
    "        subset = hash_map[hash_map[LOGG_COLUMN] == log_g]\n",
    "        x_vals = [a.value for a in subset[TEFF_COLUMN]]\n",
    "        y_vals = subset[FEH_COLUMN]\n",
    "        z_vals = subset[WEIGHT_COLUMN]\n",
    "\n",
    "        sc = axes[i].scatter(x_vals, y_vals, c=z_vals**.2, cmap='plasma', vmin=0, vmax=1)\n",
    "\n",
    "        axes[i].set_title(f\"log_g={log_g}\")\n",
    "        axes[i].set_xlabel(\"Temperature / K\")\n",
    "        axes[i].set_ylabel(\"FeHs / relative to solar\")\n",
    "        # axes[i].set_xticks(np.arange(np.min(T_effs) / u.K, np.max(T_effs) / u.K + 1, 50) * u.K)\n",
    "        # axes[i].grid()\n",
    "\n",
    "    STAR_NAME : str = \"TRAPPIST-1\"\n",
    "    cbar = fig.colorbar(sc, ax=axes, orientation='vertical', fraction=0.05, pad=0.04)\n",
    "    cbar.set_label(\"Weights\")\n",
    "    fig.suptitle(STAR_NAME)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.title(STAR_NAME)\n",
    "    \n",
    "    determined_spectrum = spectrum(spectrum_to_decompose.Wavelengths, A @ result.x)\n",
    "    plt.plot(spectrum_to_decompose.Wavelengths, spectrum_to_decompose.Fluxes, label=\"observational JWST spectrum\")\n",
    "    plt.plot(determined_spectrum.Wavelengths, determined_spectrum.Fluxes, label=\"numerically found solution (sum of spectral components)\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    residual = (determined_spectrum.Fluxes - spectrum_to_decompose.Fluxes) / spectrum_to_decompose.Fluxes\n",
    "    plt.clf()\n",
    "    plt.plot(spectrum_to_decompose.Wavelengths, residual)\n",
    "    plt.show()\n",
    "\n",
    "    return hash_map\n",
    "\n",
    "hash_map = plot_nicely(A, result, all_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694ec139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets re-run that, but with only the top few components and see if the fit is better\n",
    "\n",
    "# prolly want a new class for this lmao\n",
    "# or could be a list of phoenix spectra; maybe that would help the above code be a bit neater too\n",
    "\n",
    "ns = np.arange(1, 20, 1)\n",
    "optimalities = np.array([])\n",
    "\n",
    "def get_main_components(hash_map, number_of_top_components_to_use : int) -> list[Tuple[Quantity, Quantity, Quantity]]:\n",
    "    main_components : list[(Quantity, Quantity, Quantity)] = []\n",
    "\n",
    "    for _, row in hash_map.sort_values(WEIGHT_COLUMN, ascending=False)[0:number_of_components_to_keep].iterrows():\n",
    "        main_components.append((row[TEFF_COLUMN], row[FEH_COLUMN], row[LOGG_COLUMN]))\n",
    "    \n",
    "    return main_components\n",
    "\n",
    "for number_of_components_to_keep in tqdm(ns):\n",
    "    \n",
    "    main_components = get_main_components(hash_map, number_of_components_to_keep)\n",
    "\n",
    "    A_restricted, result_restricted = calc_result(main_components, lookup_table, len(main_components), verbose=False)\n",
    "\n",
    "    optimalities = np.append(optimalities, get_optimality(A_restricted, result_restricted)[0])\n",
    "\n",
    "# plt.clf()\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.semilogy(ns, optimalities)\n",
    "plt.semilogy(ns, [get_optimality(A, result)[0]] * len(ns), linestyle=\"dashed\", label=\"optimality when using all PHOENIX spectra\")\n",
    "plt.xticks(ns)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"number of components considered\")\n",
    "plt.ylabel(\"optimality of matrix minimisation method (lower is more optimal)\")\n",
    "plt.title(f\"using the top N components from the minimisation of all the spectra.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac576cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_components = get_main_components(hash_map, 7)\n",
    "\n",
    "shifted_lookup_table = {k: v.copy() for k, v in lookup_table.items()}\n",
    "\n",
    "for key, flux in shifted_lookup_table.items():\n",
    "    flux[:] = np.roll(flux, 0) # need the [:] for in place modification i.e. to change the reference\n",
    "\n",
    "A_restricted, result_restricted = calc_result(main_components, shifted_lookup_table, len(main_components), verbose=False)\n",
    "\n",
    "print(f\"Residual MSE = {get_optimality(A_restricted, result_restricted)[0]}\")\n",
    "\n",
    "_ = plot_nicely(A_restricted, result_restricted, main_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5358b21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can try another meta-optimiser: shift the spectrum between [-20,+20] or smthn resolutions to the left & right\n",
    "# there might be some small zero error on the phoenix grid or smthn (I wouldn't be suprised) - this would help check for that\n",
    "\n",
    "# from experimenting: no rolling is best\n",
    "main_comps = get_main_components(hash_map, 10)\n",
    "\n",
    "optimalities = []\n",
    "\n",
    "# independent copy - we dont want the shift to affect the original lookup_table\n",
    "shifted_lookup_table = {k: v.copy() for k, v in lookup_table.items()}\n",
    "\n",
    "shifts = np.arange(-10,10,1)\n",
    "\n",
    "for key, flux in shifted_lookup_table.items():\n",
    "    flux[:] = np.roll(flux, np.min(shifts)) # need the [:] for in place modification i.e. to change the reference\n",
    "\n",
    "A, result = calc_result(main_comps, shifted_lookup_table, len(main_comps), verbose=False)\n",
    "optimalities.append(get_optimality(A, result)[0])\n",
    "\n",
    "positive_shifts = shifts + np.abs(np.min(shifts))\n",
    "\n",
    "for shift in tqdm(positive_shifts[0:-1]):\n",
    "    for key, flux in shifted_lookup_table.items():\n",
    "        flux[:] = np.roll(flux, 1) # need the [:] for in place modification i.e. to change the reference\n",
    "    A, result = calc_result(main_comps, shifted_lookup_table, len(main_comps), verbose=False)\n",
    "    optimalities.append(get_optimality(A, result)[0])\n",
    "\n",
    "    # plot_nicely(A, result, main_comps)\n",
    "\n",
    "plt.clf()\n",
    "plt.semilogy(shifts, optimalities)\n",
    "plt.xlabel(\"roll / shift\")\n",
    "plt.ylabel(\"optimality of solution (using top 10 best components only)\")\n",
    "plt.show()\n",
    "\n",
    "# now we can fit in sections too\n",
    "\n",
    "# can also interpolate in different ways\n",
    "\n",
    "# also can visualise other spectra; see what works\n",
    "\n",
    "# also can use other spectra from lalitha"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spots-and-faculae-model-YvvEmflj-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
